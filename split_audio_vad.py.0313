import sys
import os
import logging
from io import BytesIO
from threading import Thread, Condition
from queue import Queue
import time
import wave
import traceback
import logging

import numpy as np
import librosa

import pygame
import webrtcvad
import sounddevice as sd

from voice_utils import RingBuffer, AudioRingBuffer,Hists, VadCounter, towave, load_wave, get_mic_devices

logger = logging.getLogger('voice')

audio_queue:Queue = Queue()
xx_callback = None

def audio_callback(data,a,b,c):
    global audio_queue
    mono_f32=data[:,0]
    mono_f32 = mono_f32.reshape(-1,1)
    audio_queue.put( mono_f32 )

def audio_transfer():
    global xx_callback
    print("Thread start")
    while xx_callback is not None:
        try:
            data = audio_queue.get(timeout=1)
            xx_callback(data)
        except:
            pass


def start_mic( *, fr=16000, bs=800, device=None, callback):
    global xx_callback
    if device is None:
        inp_dev_list = get_mic_devices(samplerate=fr, dtype=np.float32)
        device = inp_dev_list[0]['index'] if inp_dev_list and len(inp_dev_list)>0 else None
    xx_callback = callback
    ss = Thread( target=audio_transfer, daemon=True )
    ss.start()
    audioinput = sd.InputStream( samplerate=fr, blocksize=bs, device=device, dtype=np.float32, callback=audio_callback )
    audioinput.start()
    return audioinput

def rms_energy( audio, sr=16000 ):
    e = librosa.feature.rms( y=audio, hop_length=len(audio))[0][0]
    return e

class VadTbl:

    def __init__(self,size,up:int,dn:int):
        if up<dn:
            raise Exception(f"invalid parameter {size} {up} {dn}")
        self.size = size
        self.up_trigger:int = up
        self.dn_trigger:int = dn
        self.active:bool = False
        self.table:list[int] = [0] * size
        self.pos:int = 0
        self.sum:int = 0

    def add(self,value:int):
        d: int = value - self.table[self.pos]
        self.sum += d
        self.table[self.pos]=value
        self.pos = ( self.pos + 1 ) % self.size
        if self.active:
            if self.sum<=self.dn_trigger:
                self.active = False
                return True
        else:
            if self.sum>=self.up_trigger:
                self.active = True
                return True
        return False


class LowPos:
    def __init__(self, max_elements=10):
        self.max_elements = max_elements
        self.table = np.full((max_elements, 2), np.inf)
        self.current_size = 0
    def __len__(self):
        return self.current_size
    def clear(self):
        np.ndarray.fill( self.table, np.inf )
        self.current_size = 0

    def add(self, pos: int, value: int):
        # 新しい要素を挿入する位置をバイナリサーチで探す
        insert_at = np.searchsorted(self.table[:self.current_size, 0], value)
        
        # 配列がまだ最大要素数に達していない場合
        if self.current_size < self.max_elements:
            # 挿入位置以降の要素を一つ後ろにシフト
            self.table[insert_at+1:self.current_size+1] = self.table[insert_at:self.current_size]
            self.table[insert_at] = [value, pos]
            self.current_size += 1
        else:
            # 配列が満杯で、挿入位置が配列の末尾よりも前の場合
            if insert_at < self.max_elements - 1:
                # 挿入位置以降の要素を一つ後ろにシフトし、末尾の要素を削除
                self.table[insert_at+1:] = self.table[insert_at:-1]
                self.table[insert_at] = [value, pos]

    def get_table(self):
        return self.table[:self.current_size]

    def get_pos(self):
        return int(self.table[0][1])

    def remove_below_pos(self, pos: int):
        # pos以下の要素を削除する
        to_keep = self.table[:, 1] > pos
        new_pos = 0
        for i in range(self.current_size):
            if to_keep[i]:
                self.table[new_pos] = self.table[i]
                new_pos += 1
                
        # 新しい現在のサイズを更新
        self.current_size = new_pos
        
        # 残りの部分をnp.infで埋める
        if new_pos < self.max_elements:
            self.table[new_pos:] = np.inf
    
class VADx:
    """音声の区切りを検出する"""
    def __init__(self):
        # 設定
        self.fr = 16000
        self.size:int = 10
        self.up_tirg:int = 6
        self.dn_trig:int = 2
        self.vad_msec:int = 10  # 10ms,20ms,30ms
        self.vad_samples:int = int( (self.fr * self.vad_msec) / 1000 )  # 10ms,20ms,30ms
        #
        self.vad_buffer_len:int = 0
        self.vad_buffer:np.ndarray = np.zeros( self.vad_samples, dtype=np.float32)
        self.pre_buffer:RingBuffer = RingBuffer( self.vad_samples * self.size, dtype=np.float32 )
        self.seg_buffer:RingBuffer = RingBuffer( self.fr * 30, dtype=np.float32 )
        self.seg_dict:dict = None
        # 判定用 カウンタとフラグ
        self.rec=False
        # 処理用
        self.count1:VadTbl = VadTbl( self.size, up=self.up_tirg, dn=self.dn_trig )
        self.count2:VadTbl = VadTbl( 4,3,0 )
        self.last_down:LowPos = LowPos()
        self.max_speech_length = 5 * self.fr
        self.seg = b''
        self.vad = webrtcvad.Vad()
        # zero crossing
        self.zc_count:VadTbl = VadTbl( self.size, up=self.up_tirg, dn=self.dn_trig )
        # segments
        self.num_samples = 0
        self.dict_list:list[dict] = []
        #
        self.hists:Hists = Hists( self.seg_buffer.capacity )

    def audio_callback(self, raw_audio:np.ndarray, *args ) ->bool:
        try:
            buffer:np.ndarray = self.vad_buffer
            buffer_len:int = self.vad_buffer_len
            mono_f32 = raw_audio[:,0]
            mono_len = len(mono_f32)
            mono_pos = 0
            while True:
                nn = min( mono_len-mono_pos, self.vad_samples - buffer_len )
                np.copyto( buffer[buffer_len:buffer_len+nn], mono_f32[mono_pos:mono_pos+nn])
                buffer_len += nn
                mono_pos+=nn

                if buffer_len<self.vad_samples:
                    break
                self.audio_callback2( buffer )
                self.num_samples + buffer_len
                buffer_len = 0
            self.vad_buffer_len = buffer_len
        except:
            traceback.print_exc()

    def audio_callback2(self, frame:np.ndarray ) ->bool:
        try:
            # mono_f32 = raw_audio[:,0]
            # mono_len = len(mono_f32)
            # mono_pos = 0
            num_samples = self.num_samples
            # self.num_samples+=mono_len
            # while True:
            #     nn = min( mono_len-mono_pos, self.vad_samples - self.vad_buffer_len )
            #     np.copyto( self.vad_buffer[self.vad_buffer_len:self.vad_buffer_len+nn], mono_f32[mono_pos:mono_pos+nn])
            #     self.vad_buffer_len += nn
            #     mono_pos+=nn
            #     num_samples+=nn

            #     if self.vad_buffer_len<self.vad_samples:
            #         break
                
            pcm = frame * 32767.0
            pcm = pcm.astype(np.int16)
            pcm_bytes = pcm.tobytes()

            is_speech = 1 if self.vad.is_speech( pcm_bytes, self.fr ) else 0
            self.count1.add(is_speech)
            self.count2.add(is_speech)
            zz = librosa.zero_crossings(frame)
            zc = sum(zz)
            self.zc_count.add( zc )
            #
            self.hists.add( frame.max(), frame.min(), self.count1.sum, self.count2.sum, zc )
            # rms_energy(frame, sr=self.fr )
            if self.rec:
                if self.count1.sum<=5:
                    self.last_down.add( len(self.seg_buffer ), self.count1.sum )
                self.seg_buffer.append( frame )
                if not self.count1.active or ( len(self.seg_buffer)>self.max_speech_length and len(self.last_down)>0 ):
                    if not self.count1.active:
                        split_len = len(self.seg_buffer)
                    else:
                        split_len = self.last_down.get_pos()
                    st = self.seg_buffer.offset
                    start_sec = st/self.fr
                    ed = st + split_len
                    end_sec = ed/self.fr
                    audio = self.seg_buffer[:split_len].copy()
                    self.seg_dict['start'] = start_sec
                    self.seg_dict['end_zc'] = [zc,self.zc_count.sum]
                    self.seg_dict['end'] = end_sec
                    self.seg_dict['audio'] = audio
                    self.seg_dict['hists'] = self.hists.to_numpy(0,split_len//self.vad_samples)
                    self.dict_list.append( self.seg_dict )
                    if not self.count1.active:
                        self.seg_dict = None
                        self.seg_buffer.clear()
                        self.pre_buffer.clear()
                        self.hists.clear()
                        self.rec=False
                    else:
                        self.seg_buffer.remove(split_len)
                        self.hists.remove(split_len//self.vad_samples)
                        self.seg_dict = { 'start_zc': [zc,self.zc_count.sum]}
                        self.last_down.remove_below_pos(split_len)
            else:
                if self.count1.active:
                    self.seg_dict = { 'start_zc': [zc,self.zc_count.sum]}
                    self.rec=True
                    self.last_down.clear()
                    self.seg_buffer.clear()
                    self.seg_buffer.append( self.pre_buffer[:].copy() )
                    self.seg_buffer.append( frame )
                    self.hists.keep( len(self.seg_buffer)//self.vad_samples )
                    self.seg_buffer.offset = num_samples - len(self.seg_buffer)
                    self.pre_buffer.clear()
                else:
                    self.pre_buffer.append(frame)


        except:
            traceback.print_exc()

    def end(self):
        if self.rec:
            st = self.seg_buffer.offset
            start_sec = st/self.fr
            ed = st + len(self.seg_buffer)
            end_sec = ed/self.fr
            audio = self.seg_buffer[:]
            self.dict_list.append( { 'start':start_sec, 'end':end_sec, 'audio':audio})
    
class WhisperSTT:

    def __init__(self):
        self.fr=16000
        self.num_fr = 0
        self.dict_list:list[dict] = []
       
        self.vad:VadCounter = VadCounter()
        self.vad_buffer:RingBuffer = RingBuffer( self.vad.size, dtype=np.float32 )
        self.rec:bool=False
        self.last_break_fr = 0
        self.last_transcrib_fr = 0
        self.transcrib_elaps_fr = 0

        self.buffer_start = 0
        self.buffer_fr=0
        self.buffer:AudioRingBuffer = AudioRingBuffer( sec=30, fr=self.fr )
        self.next:int = self.fr*5

    def load(self):
        print( f"#Load model")

        librosa.resample( np.zeros( (1,1), dtype=np.float32), orig_sr=44100, target_sr=16000 ) # preload of librosa

        model_size = "large-v3"
        model_size = "medium"
        # Run on GPU with FP16
        # vself.whisper_model = WhisperModel(model_size, device="cuda", compute_type="float16")
        # or run on GPU with INT8
        # self.whisper_model:WhisperModel = WhisperModel(model_size, device="cuda", compute_type="int8_float16")
        # or run on CPU with INT8
        # self.whisper_model = WhisperModel(model_size, device="cpu", compute_type="int8")

    def audio_callback(self,raw_data):


        # マイクからの入力はコピーして使うこと！
        fr1 = self.buffer.get_last_fr()
        self.buffer.append( raw_data[:,0] )
        fr2 = self.buffer.get_last_fr()

        a,b,c,d = self.vad.put_f32( raw_data[:,0] )
        if not a or c or not d:
            self.last_break_fr = fr2

        rec_min_fr = int( 2 * self.fr )
        stop_fr = int( 2 * self.fr )
        rec_split_fr = int( 10 * self.fr)
        keep_startup_fr = int( 1 * self.fr )

        if self.rec:
            # 録音中
            if not a and not b and not d:
                # 無音区間検出
                silent_length = 0
                if self.silent_start_fr<0:
                    self.silent_start_fr = fr2
                    if len(self.buffer)>=rec_min_fr:
                        print( f"[Rec]--silent--split")
                        self.run_transcribe_time()
                    else:
                        print( f"[Rec]--silent--")
                else:
                    silent_length = fr2 - self.silent_start_fr
                    if silent_length>=stop_fr:
                        if self.last_transcrib_fr<self.silent_start_fr:
                            print( f"[Rec]--silent-- cleanup")
                            self.run_transcribe_time()
                        self.rec = False
                        print( f"[Rec]Stop")
            else:
                self.silent_start_fr = -1
                # 録音中に長くなりすぎたら区切る
                if len(self.buffer)>rec_split_fr:
                    print( f"[Rec]Split")
                    self.run_transcribe_time()
        else:
            # 停止中...
            if not a or c or not d:
                print( f"[Rec]start")
                # 音声っぽいのを検出
                clear = len(self.buffer)-keep_startup_fr
                if clear>0:
                    self.buffer.remove(clear)
                self.rec = True
                self.silent_start_fr=-1

    def run_transcribe_time(self):
            t1 = time.time()
            self.last_transcrib_fr = self.buffer.get_last_fr()
            self.run_transcribe()
            t2 = time.time()
            self.transcrib_elaps_fr = int( ((t2-t1+1.0)*self.fr + self.transcrib_elaps_fr ) / 2 )

    def run_transcribe(self):
        print( f"run_transcribe" )
        #wav = self.buffer.towave()
        audio = self.buffer[0:]
        tt0=time.time()
        segments, info = self.whisper_model.transcribe( audio, beam_size=1, best_of=2, temperature=0, language='ja', condition_on_previous_text='まいど！' )
        # print("Detected language '%s' with probability %f" % (info.language, info.language_probability))
        cut_fr = 0
        for segment in segments:
            seg_start_fr = int(segment.start*self.fr)
            fr_start = seg_start_fr + self.buffer.offset
            start_sec = fr_start/self.fr

            seg_end_fr = int(segment.end*self.fr)
            fr_end = seg_end_fr + self.buffer.offset
            end_sec = fr_end/self.fr

            print( f" detect {segment.id} {start_sec:.3f}s-{end_sec:.3f}s {segment.start:.3f}s-{segment.end:.3f}s {seg_start_fr} {seg_end_fr} {segment.text}" )
            cut_fr = seg_start_fr
        tt1 = time.time()
        print( f"  {tt1-tt0}(sec)")
        if cut_fr>0:
            self.buffer.remove(cut_fr)

import matplotlib.pyplot as plt
import librosa
import librosa.display

def analyze_audio2(audio_data, sr):
    # Figureと複数のAxesを作成
    fig, axs = plt.subplots(nrows=4, ncols=1, figsize=(12, 8), sharex=True)

    # 波形のプロット
    librosa.display.waveshow(audio_data, sr=sr, ax=axs[0],color='blue')
    axs[0].set_title('Waveform')

    # RMSエナジー
    rms_energy = librosa.feature.rms(y=audio_data)[0]
    axs[1].semilogy(rms_energy, label='RMS Energy')
    axs[1].legend()
    axs[1].set_title('RMS Energy')

    # ゼロ交差数
    zero_crossings = librosa.feature.zero_crossing_rate(audio_data)[0]
    axs[2].plot(zero_crossings, label='Zero Crossing Rate')
    axs[2].legend()
    axs[2].set_title('Zero Crossing Rate')

    # 基本周波数（F0）
    f0, voiced_flag, voiced_probs = librosa.pyin(audio_data, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))
    times = librosa.times_like(f0)
    axs[3].plot(times, f0, label='F0', color='green')
    axs[3].legend()
    axs[3].set_title('Fundamental Frequency (F0)')

    # 各Axesの設定
    for ax in axs:
        ax.label_outer()

    plt.tight_layout()

    # Figureオブジェクトを返す
    return fig

def analyze_audio(audio_data, sr):
    # Figureと複数のAxesを作成
    fig, axs = plt.subplots(nrows=4, ncols=1, figsize=(12, 8), sharex=True)

    # 波形のプロット
    librosa.display.waveshow(audio_data, sr=sr, ax=axs[0], color='blue')
    axs[0].set_title('Waveform')

    # RMSエナジー
    rms_energy = librosa.feature.rms(y=audio_data)[0]
    frames = np.arange(len(rms_energy))
    t_rms = librosa.frames_to_time(frames, sr=sr)
    axs[1].semilogy(t_rms, rms_energy, label='RMS Energy', color='orange')
    axs[1].legend()
    axs[1].set_title('RMS Energy')

    # ゼロ交差数
    zero_crossings = librosa.feature.zero_crossing_rate(audio_data)[0]
    frames = np.arange(len(zero_crossings))
    t_zcr = librosa.frames_to_time(frames, sr=sr)
    axs[2].plot(t_zcr, zero_crossings, label='Zero Crossing Rate', color='purple')
    axs[2].legend()
    axs[2].set_title('Zero Crossing Rate')

    # 基本周波数（F0）
    f0, voiced_flag, voiced_probs = librosa.pyin(audio_data, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))
    t_f0 = librosa.times_like(f0, sr=sr)
    axs[3].plot(t_f0, f0, label='F0', color='green')
    axs[3].legend()
    axs[3].set_title('Fundamental Frequency (F0)')

    # 各Axesの設定
    for ax in axs:
        ax.label_outer()

    plt.tight_layout()

    # Figureオブジェクトを返す
    return fig

import matplotlib.pyplot as plt
import numpy as np

def plot_hists_data(hists, sr, framesize):
    # Figureと5つのAxesを作成
    fig, axs = plt.subplots(nrows=5, ncols=1, figsize=(12, 10), sharex=True)

    # サンプルレートに基づいて時間軸を計算
    frames = np.arange(hists.shape[1]) * framesize
    t = frames / sr

    # 最高値と最低値のプロット
    hi = hists[0]
    lo = hists[1]
    axs[0].plot(t, hi, label='Highest Value', color='blue')
    axs[0].plot(t, lo, label='Lowest Value', color='red')
    axs[0].legend()
    axs[0].set_title('Highest and Lowest Values')

    # webrtcvadの結果1と結果2のプロット
    vad1 = hists[2]
    vad2 = hists[3]
    axs[1].plot(t, vad1, label='VAD1 Result', color='orange')
    axs[1].plot(t, vad2, label='VAD2 Result', color='purple')
    axs[1].legend()
    axs[1].set_title('WebRTC VAD Results')

    # ゼロ交差数のプロット
    zc = hists[4]
    axs[2].plot(t, zc, label='Zero Crossing Rate', color='green')
    axs[2].legend()
    axs[2].set_title('Zero Crossing Rate')

    # 各Axesの設定
    for ax in axs[:-1]:
        ax.label_outer()
    axs[-1].set_xlabel('Time (s)')

    plt.tight_layout()

    # Figureオブジェクトを返す
    return fig

def main():
    wav_filename='testData/nakagawke01.wav'
    #wav_filename='testData/voice_mosimosi.wav'
    #wav_filename='testData/voice_command.wav'

    print( f"#Split audio")
    #stt:WhisperSTT = WhisperSTT()
    #stt.load()

    stt = VADx()
    load_wave( wav_filename, callback=stt.audio_callback, wait=False )
    stt.end()
    # audio = start_mic( callback=stt.audio_callback )

    # Pygameの初期化
    pygame.init()
    pygame.mixer.init()
    n=-1
    selected=-1
    while True:
        if n<0:
            for idx,item in enumerate(stt.dict_list):
                mark = "*" if idx==selected else " "
                s = item.get('start',-1)
                e = item.get('end',-1)
                zc1 = item.get('start_zc',[])
                zc1a = f"{zc1[0]:3d}" if isinstance(zc1,list) and len(zc1)>0 else "---"
                zc1b = f"{zc1[1]:3d}" if isinstance(zc1,list) and len(zc1)>1 else "---"
                zc2 = item.get('end_zc',[])
                zc2a = f"{zc2[0]:3d}" if isinstance(zc2,list) and len(zc2)>0 else "---"
                zc2b = f"{zc2[1]:3d}" if isinstance(zc2,list) and len(zc2)>1 else "---"
                txt = item.get('content','')
                print( f"{mark} {idx:3d} {zc1b} {zc1a} {s:8.3f} - {e:8.3f} {zc2a} {zc2b} : {txt}" )
        mark = f"[{selected:3d}]" if selected>0 else "[---]"
        keyin = input(f"[{mark} >> ")
        try:
            n = int(keyin)
        except:
            n=-1
        if 0<=n and n<len(stt.dict_list):
            dict = stt.dict_list[n]
            au = dict.get('audio')
            if selected != n:
                selected = n
                plt.close('all')
                fig = dict.get('fig')
                if fig is None:
                    hists = dict['hists']
                    fig = dict['fig'] = plot_hists_data( hists, sr=16000, framesize=stt.vad_samples )
                    #fig = dict['fig'] = analyze_audio(au, sr=16000)
                fig.show()
            wav = towave( au )
            # オンメモリのwaveデータを読み込む
            wave_sound = pygame.mixer.Sound(wav)
            # 再生
            wave_sound.play(fade_ms=0)

def tbl_test():
    tbl:VadTbl = VadTbl(5, up=4, dn=1 )
    
    hist:list = []
    sgn=1
    for ai in range(1,60,3):
        i = ai * sgn
        sgn = 1 if sgn<0 else -1
        hist.append(i)
        tbl.add(i)
        s = sum(hist[-tbl.size:])
        print( f"sum:{tbl.sum} {s}")

if __name__ == "__main__":
    # tbl_test()
    main()